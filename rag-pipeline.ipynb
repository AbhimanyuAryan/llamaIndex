{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\n",
    "    # input_files = [\"./docs/colbert.pdf\"]\n",
    "    input_files = [\"./docs/PLAID.pdf\", \"./docs/colbert.pdf\", \"./docs/colbert-v2.pdf\"]\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> \n",
      "\n",
      "40 \n",
      "\n",
      "<class 'llama_index.core.schema.Document'>\n",
      "Doc ID: 1d86c135-0772-49a6-b01e-306abc1aa60d\n",
      "Text: PLAID: An Efficient Engine for Late Interaction Retrieval Keshav\n",
      "Santhanam∗ keshav2@stanford.edu Stanford University United StatesOmar\n",
      "Khattab∗ okhattab@stanford.edu Stanford University United States\n",
      "Christopher Potts Stanford University United StatesMatei Zaharia\n",
      "Stanford University United States ABSTRACT Pre-trained language models\n",
      "are increas...\n"
     ]
    }
   ],
   "source": [
    "print(type(documents), \"\\n\")\n",
    "print(len(documents), \"\\n\")\n",
    "print(type(documents[0]))\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all documents length with a delay i.e. clear the screen after printing each document\n",
    "# import time\n",
    "# for doc in documents:\n",
    "#     print(doc)\n",
    "#     time.sleep(1)\n",
    "#     os.system('clear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc ID: 58053497-9aea-42bd-9aa3-db0f570f0097\n",
      "Text: PLAID: An Efficient Engine for Late Interaction Retrieval Keshav\n",
      "Santhanam∗ keshav2@stanford.edu Stanford University United StatesOmar\n",
      "Khattab∗ okhattab@stanford.edu Stanford University United States\n",
      "Christopher Potts Stanford University United StatesMatei Zaharia\n",
      "Stanford University United States ABSTRACT Pre-trained language models\n",
      "are increas...\n"
     ]
    }
   ],
   "source": [
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0j/d0xd373d2tb7zdzcsqrk3mpw0000gn/T/ipykernel_24193/339795532.py:6: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import ServiceContext\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm, embed_model=\"local:BAAI/bge-small-en-v1.5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    [document],\n",
    "    service_context=service_context\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The query encoder preprocesses textual queries by tokenizing them into BERT-based WordPiece tokens and prepending a special token [Q] to indicate it is a query. The tokenized query is then passed through BERT's deep transformer architecture to compute contextualized representations of each token. Query augmentation is applied by padding the query with BERT's special [mask] tokens to allow for learning to expand or re-weigh terms based on importance. The output embeddings are passed through a linear layer for dimension control and normalization.\n",
      "\n",
      "On the other hand, the document encoder segments a document into tokens and prepends BERT's start token [CLS] followed by a special token [D] to indicate it is a document. Unlike queries, no [mask] tokens are appended to documents. After processing through BERT and a linear layer, the document encoder filters out embeddings corresponding to punctuation symbols. This filtering step aims to reduce the number of embeddings per document by excluding punctuation embeddings, which are deemed unnecessary for effectiveness.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"explain the query and document encoder. Then show differences between them\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\n",
    "    \"explain difference between query and document encoder visually\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The query encoder preprocesses textual queries by adding a special token [Q] at the beginning and padding with [mask] tokens if needed. It then passes the sequence through BERT and a linear layer. On the other hand, the document encoder segments documents into tokens, adds a start token [CLS] followed by [D], and filters out embeddings corresponding to punctuation symbols. The document encoder does not use [mask] tokens like the query encoder. Both encoders ultimately produce bags of embeddings for queries and documents, respectively.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
