{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\n",
    "    input_files = [\"./docs/PLAID.pdf\"]\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> \n",
      "\n",
      "10 \n",
      "\n",
      "<class 'llama_index.core.schema.Document'>\n",
      "Doc ID: d6151871-a842-4fa6-bb5c-c0c9cfd5b5b2\n",
      "Text: PLAID: An Efficient Engine for Late Interaction Retrieval Keshav\n",
      "Santhanam‚àó keshav2@stanford.edu Stanford University United StatesOmar\n",
      "Khattab‚àó okhattab@stanford.edu Stanford University United States\n",
      "Christopher Potts Stanford University United StatesMatei Zaharia\n",
      "Stanford University United States ABSTRACT Pre-trained language models\n",
      "are increas...\n"
     ]
    }
   ],
   "source": [
    "print(type(documents), \"\\n\")\n",
    "print(len(documents), \"\\n\")\n",
    "print(type(documents[0]))\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc ID: d6151871-a842-4fa6-bb5c-c0c9cfd5b5b2\n",
      "Text: PLAID: An Efficient Engine for Late Interaction Retrieval Keshav\n",
      "Santhanam‚àó keshav2@stanford.edu Stanford University United StatesOmar\n",
      "Khattab‚àó okhattab@stanford.edu Stanford University United States\n",
      "Christopher Potts Stanford University United StatesMatei Zaharia\n",
      "Stanford University United States ABSTRACT Pre-trained language models\n",
      "are increas...\n",
      "\u001b[H\u001b[2JDoc ID: eec9fb0e-e4a5-4923-8de7-b1f671c546f4\n",
      "Text: PLAID, May 2022, Preprint Keshav Santhanam*, Omar Khattab*,\n",
      "Christopher Potts, and Matei Zaharia Question Passage Question Encoder\n",
      "Passage EncoderMaxSim MaxSim MaxSimscore Offline Indexing Figure 1:\n",
      "The late interaction architecture, given a query and a passage.\n",
      "Diagram from Khattab et al. [21] with permission. interaction with the\n",
      "residual repr...\n",
      "\u001b[H\u001b[2JDoc ID: 531d8b11-edee-4f1b-8ae1-97fd2cc9ec67\n",
      "Text: PLAID: An Efficient Engine for Late Interaction Retrieval PLAID,\n",
      "May 2022, Preprint MS MARCO v1 LoTTE pooled 20 40 60 80100 120 140 #\n",
      "Passages0.600.700.800.900.951.00Recall of top 10 (a)ùëò=10 200 400 600\n",
      "800 1000 # Passages0.600.700.800.900.951.00Recall of top 100  (b)ùëò=100\n",
      "2000 4000 6000 8000 10000 # Passages0.600.700.800.900.951.00Recall of\n",
      "top...\n",
      "\u001b[H\u001b[2JDoc ID: 9de9efbd-0950-4c93-b57e-5de3b30e4623\n",
      "Text: PLAID, May 2022, Preprint Keshav Santhanam*, Omar Khattab*,\n",
      "Christopher Potts, and Matei Zaharia 3.3 Centroids Alone Identify\n",
      "Strong Candidates This breakdown in Figure 2b demonstrates that\n",
      "exhaustively scoring a large number of candidates passages,\n",
      "particularly gathering and decompressing their residuals, can amount\n",
      "to a considerable cost. Wher...\n",
      "\u001b[H\u001b[2JDoc ID: 077cace2-a850-4ed3-8951-ea96dc82c0ff\n",
      "Text: PLAID: An Efficient Engine for Late Interaction Retrieval PLAID,\n",
      "May 2022, Preprint TopK(ndocs)MaxSimPrune < tcs Stage 2: Centroid\n",
      "Interaction with PruningCentroid ScoresApprox. Relevance ScoresTopK\n",
      "(ndocs4)MaxSimStage 3: Centroid Interaction w/out PruningCentroid\n",
      "ScoresApprox. Relevance ScoresCentroidsQuery EmbeddingsCentroid\n",
      "ScoresTopK(nprobe)...\n",
      "\u001b[H\u001b[2JDoc ID: e8b461ec-a767-4736-9373-5094dcb19d05\n",
      "Text: PLAID, May 2022, Preprint Keshav Santhanam*, Omar Khattab*,\n",
      "Christopher Potts, and Matei Zaharia Dataset # Passages # Tokens #\n",
      "QueriesColBERTv2 Index Size (GiB) Vanilla PLAID MS MARCO v1 [36] 8.8M\n",
      "597.9M 6980 24.6 21.6 Wikipedia [18] 21.0M 2.6B 8757 105.2 92.0 LoTTE\n",
      "pooled [42] 2.4M 339.4M 2931 14.0 12.3 MS MARCO v2 [6] 138.4M 9.4B\n",
      "3903 246.0 20...\n",
      "\u001b[H\u001b[2JDoc ID: 2e89d604-7e0e-4792-b0be-7e2b2f7198fc\n",
      "Text: PLAID: An Efficient Engine for Late Interaction Retrieval PLAID,\n",
      "May 2022, Preprint across 3 trials. For other results we describe the\n",
      "specific measure- ment procedure in the relevant section. We discard\n",
      "the query en- coding latency for neural models (ColBERTv1 [ 22],\n",
      "vanilla Col- BERTv2 [ 42], PLAID ColBERTv2, and SPLADEv2 [ 10])\n",
      "following Mack...\n",
      "\u001b[H\u001b[2JDoc ID: 8402c0fc-823e-4728-8047-215c87994616\n",
      "Text: PLAID, May 2022, Preprint Keshav Santhanam*, Omar Khattab*,\n",
      "Christopher Potts, and Matei Zaharia 0.0 2.5 5.0 7.5 Speedup+ Fast\n",
      "Decompression+ Centroid Pruning+ Centroid InteractionVanilla ColBERTv2\n",
      "6.6x5.2x3.7x1.0x (a) GPU. 0 15 30 45 Speedup+ Fast Kernels+ Centroid\n",
      "Pruning+ Centroid InteractionVanilla ColBERTv2 42.4x8.6x4.2x1.0x (b)\n",
      "CPU (8 thre...\n",
      "\u001b[H\u001b[2JDoc ID: 88e2aa9f-0aab-419d-ae64-0fdaafc213f2\n",
      "Text: PLAID: An Efficient Engine for Late Interaction Retrieval PLAID,\n",
      "May 2022, Preprint filtering candidate passages. We showed that\n",
      "retrieval with only ColBERTv2 centroids retains high recall compared\n",
      "to vanilla Col- BERTv2, and the distribution of centroid relevance\n",
      "scores skews toward lower magnitude scores. Using these insights, we\n",
      "introduced th...\n",
      "\u001b[H\u001b[2JDoc ID: 022237e0-e8a4-43c4-aea2-b455d27a9e58\n",
      "Text: PLAID, May 2022, Preprint Keshav Santhanam*, Omar Khattab*,\n",
      "Christopher Potts, and Matei Zaharia Training Approach to Dense\n",
      "Passage Retrieval for Open-Domain Question An- swering. In Proceedings\n",
      "of the 2021 Conference of the North American Chapter of the\n",
      "Association for Computational Linguistics: Human Language Technolo-\n",
      "gies. Association for Co...\n",
      "\u001b[H\u001b[2J"
     ]
    }
   ],
   "source": [
    "# print all documents length with a delay i.e. clear the screen after printing each document\n",
    "import time\n",
    "for doc in documents:\n",
    "    print(doc)\n",
    "    time.sleep(1)\n",
    "    os.system('clear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc ID: b7e1bf98-d0ce-41ec-bd81-9d1ba792b16b\n",
      "Text: PLAID: An Efficient Engine for Late Interaction Retrieval Keshav\n",
      "Santhanam‚àó keshav2@stanford.edu Stanford University United StatesOmar\n",
      "Khattab‚àó okhattab@stanford.edu Stanford University United States\n",
      "Christopher Potts Stanford University United StatesMatei Zaharia\n",
      "Stanford University United States ABSTRACT Pre-trained language models\n",
      "are increas...\n"
     ]
    }
   ],
   "source": [
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0j/d0xd373d2tb7zdzcsqrk3mpw0000gn/T/ipykernel_24193/339795532.py:6: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(\n",
      "/Users/abhi/miniconda3/envs/RAG/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import ServiceContext\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm, embed_model=\"local:BAAI/bge-small-en-v1.5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    [document],\n",
    "    service_context=service_context\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stages in the PLAID pipeline are as follows:\n",
      "1. Centroid interaction without pruning\n",
      "2. Centroid interaction with centroid pruning\n",
      "3. Optimized kernels for efficient deployment of centroid interaction and MaxSim operations\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"what are stages in PLAID pipeline?\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
